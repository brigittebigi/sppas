
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>calculus.stats.linregress &#8212; SPPAS 1.9.8 documentation</title>
    <link rel="stylesheet" href="../../../_static/nature.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../../_static/doctools.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 
  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="../../../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="nav-item nav-item-0"><a href="../../../index.html">SPPAS 1.9.8 documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="../../index.html" accesskey="U">Module code</a> &#187;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <h1>Source code for calculus.stats.linregress</h1><div class="highlight"><pre>
<span></span><span class="c1"># -*- coding: UTF-8 -*-</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    ..</span>
<span class="sd">        ---------------------------------------------------------------------</span>
<span class="sd">         ___   __    __    __    ___</span>
<span class="sd">        /     |  \  |  \  |  \  /              the automatic</span>
<span class="sd">        \__   |__/  |__/  |___| \__             annotation and</span>
<span class="sd">           \  |     |     |   |    \             analysis</span>
<span class="sd">        ___/  |     |     |   | ___/              of speech</span>

<span class="sd">        http://www.sppas.org/</span>

<span class="sd">        Use of this software is governed by the GNU Public License, version 3.</span>

<span class="sd">        SPPAS is free software: you can redistribute it and/or modify</span>
<span class="sd">        it under the terms of the GNU General Public License as published by</span>
<span class="sd">        the Free Software Foundation, either version 3 of the License, or</span>
<span class="sd">        (at your option) any later version.</span>

<span class="sd">        SPPAS is distributed in the hope that it will be useful,</span>
<span class="sd">        but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="sd">        MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="sd">        GNU General Public License for more details.</span>

<span class="sd">        You should have received a copy of the GNU General Public License</span>
<span class="sd">        along with SPPAS. If not, see &lt;http://www.gnu.org/licenses/&gt;.</span>

<span class="sd">        This banner notice must not be removed.</span>

<span class="sd">        ---------------------------------------------------------------------</span>

<span class="sd">    src.calculus.stats.linregress.py</span>
<span class="sd">    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>

<span class="sd">    :author:       Brigitte Bigi</span>
<span class="sd">    :organization: Laboratoire Parole et Langage, Aix-en-Provence, France</span>
<span class="sd">    :contact:      develop@sppas.org</span>
<span class="sd">    :license:      GPL, v3</span>
<span class="sd">    :copyright:    Copyright (C) 2011-2018  Brigitte Bigi</span>

<span class="sd">The goal of linear regression is to fit a line to a set of points.</span>
<span class="sd">Equation of the line is y = mx + b</span>
<span class="sd">where m is slope, b is y-intercept.</span>

<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">from</span> <span class="nn">.central</span> <span class="k">import</span> <span class="n">fmean</span>
<span class="kn">from</span> <span class="nn">.central</span> <span class="k">import</span> <span class="n">fsum</span>

<span class="c1"># ---------------------------------------------------------------------------</span>


<div class="viewcode-block" id="compute_error_for_line_given_points"><a class="viewcode-back" href="../../../rst/calculus.stats.html#calculus.compute_error_for_line_given_points">[docs]</a><span class="k">def</span> <span class="nf">compute_error_for_line_given_points</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">points</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Error function (also called a cost function).</span>

<span class="sd">    It measures how &quot;good&quot; a given line is.</span>

<span class="sd">    This function will take in a (m,b) pair and return an</span>
<span class="sd">    error value based on how well the line fits our data.</span>
<span class="sd">    To compute this error for a given line, we&#39;ll iterate through each (x,y)</span>
<span class="sd">    point in our data set and sum the square distances between each point&#39;s y</span>
<span class="sd">    value and the candidate line&#39;s y value (computed at mx + b).</span>

<span class="sd">    Lines that fit our data better (where better is defined by our error</span>
<span class="sd">    function) will result in lower error values.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">total_error</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">points</span><span class="p">:</span>
        <span class="n">total_error</span> <span class="o">+=</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="p">(</span><span class="n">m</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">b</span><span class="p">))</span> <span class="o">**</span> <span class="mi">2</span>
    <span class="k">return</span> <span class="n">total_error</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">points</span><span class="p">))</span></div>

<span class="c1"># ---------------------------------------------------------------------------</span>


<div class="viewcode-block" id="step_gradient"><a class="viewcode-back" href="../../../rst/calculus.stats.html#calculus.step_gradient">[docs]</a><span class="k">def</span> <span class="nf">step_gradient</span><span class="p">(</span><span class="n">b_current</span><span class="p">,</span> <span class="n">m_current</span><span class="p">,</span> <span class="n">points</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;One step of a gradient linear regression.</span>

<span class="sd">    To run gradient descent on an error function, we first need to compute</span>
<span class="sd">    its gradient. The gradient will act like a compass and always point us</span>
<span class="sd">    downhill. To compute it, we will need to differentiate our error function.</span>
<span class="sd">    Since our function is defined by two parameters (m and b), we will need</span>
<span class="sd">    to compute a partial derivative for each.</span>

<span class="sd">    Each iteration will update m and b to a line that yields slightly lower</span>
<span class="sd">    error than the previous iteration.</span>

<span class="sd">    The learning_rate variable controls how large of a step we take downhill</span>
<span class="sd">    during each iteration. If we take too large of a step, we may step over</span>
<span class="sd">    the minimum. However, if we take small steps, it will require many</span>
<span class="sd">    iterations to arrive at the minimum.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">b_gradient</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">m_gradient</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">points</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">points</span><span class="p">:</span>
        <span class="n">b_gradient</span> <span class="o">+=</span> <span class="o">-</span><span class="p">(</span><span class="mf">2.</span><span class="o">/</span><span class="n">n</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="p">((</span><span class="n">m_current</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">b_current</span><span class="p">))</span>
        <span class="n">m_gradient</span> <span class="o">+=</span> <span class="o">-</span><span class="p">(</span><span class="mf">2.</span><span class="o">/</span><span class="n">n</span><span class="p">)</span> <span class="o">*</span> <span class="n">x</span> <span class="o">*</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="p">((</span><span class="n">m_current</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">b_current</span><span class="p">))</span>
    <span class="n">new_b</span> <span class="o">=</span> <span class="n">b_current</span> <span class="o">-</span> <span class="p">(</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">b_gradient</span><span class="p">)</span>
    <span class="n">new_m</span> <span class="o">=</span> <span class="n">m_current</span> <span class="o">-</span> <span class="p">(</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">m_gradient</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">[</span><span class="n">new_b</span><span class="p">,</span> <span class="n">new_m</span><span class="p">]</span></div>

<span class="c1"># ---------------------------------------------------------------------------</span>


<div class="viewcode-block" id="gradient_descent"><a class="viewcode-back" href="../../../rst/calculus.stats.html#calculus.gradient_descent">[docs]</a><span class="k">def</span> <span class="nf">gradient_descent</span><span class="p">(</span><span class="n">points</span><span class="p">,</span>
                     <span class="n">starting_b</span><span class="p">,</span> <span class="n">starting_m</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span> <span class="n">num_iterations</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Gradient descent is an algorithm that minimizes functions.</span>

<span class="sd">    Given a function defined by a set of parameters, gradient descent starts</span>
<span class="sd">    with an initial set of parameter values and iteratively moves toward a set</span>
<span class="sd">    of parameter values that minimize the function. This iterative minimization</span>
<span class="sd">    is achieved using calculus, taking steps in the negative direction of</span>
<span class="sd">    the function gradient.</span>

<span class="sd">    :param points: a list of tuples (x,y) of float values.</span>
<span class="sd">    :param starting_b: (float)</span>
<span class="sd">    :param starting_m: (float)</span>
<span class="sd">    :param learning_rate: (float)</span>
<span class="sd">    :param num_iterations: (int)</span>
<span class="sd">    :returns: intercept, slope</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">points</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="mf">0.</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">starting_b</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">starting_m</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_iterations</span><span class="p">):</span>
        <span class="n">b</span><span class="p">,</span> <span class="n">m</span> <span class="o">=</span> <span class="n">step_gradient</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">points</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">b</span><span class="p">,</span> <span class="n">m</span></div>

<span class="c1"># ---------------------------------------------------------------------------</span>


<div class="viewcode-block" id="gradient_descent_linear_regression"><a class="viewcode-back" href="../../../rst/calculus.stats.html#calculus.gradient_descent_linear_regression">[docs]</a><span class="k">def</span> <span class="nf">gradient_descent_linear_regression</span><span class="p">(</span><span class="n">points</span><span class="p">,</span> <span class="n">num_iterations</span><span class="o">=</span><span class="mi">50000</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Gradient descent method for linear regression.</span>

<span class="sd">    adapted from:</span>
<span class="sd">    http://spin.atomicobject.com/2014/06/24/gradient-descent-linear-regression/</span>

<span class="sd">    :param points: a list of tuples (x,y) of float values.</span>
<span class="sd">    :param num_iterations: (int)</span>
<span class="sd">    :returns: intercept, slope</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">g</span> <span class="o">=</span> <span class="n">gradient_descent</span><span class="p">(</span><span class="n">points</span><span class="p">,</span>
                         <span class="n">starting_b</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span>  <span class="c1"># initial y-intercept guess</span>
                         <span class="n">starting_m</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span>  <span class="c1"># initial slope guess</span>
                         <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">,</span>
                         <span class="n">num_iterations</span><span class="o">=</span><span class="n">num_iterations</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">g</span></div>

<span class="c1"># ---------------------------------------------------------------------------</span>


<div class="viewcode-block" id="tga_linear_regression"><a class="viewcode-back" href="../../../rst/calculus.stats.html#calculus.tga_linear_regression">[docs]</a><span class="k">def</span> <span class="nf">tga_linear_regression</span><span class="p">(</span><span class="n">points</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Linear regression as proposed in TGA, by Dafydd Gibbon.</span>

<span class="sd">    http://wwwhomes.uni-bielefeld.de/gibbon/TGA/</span>

<span class="sd">    :param points: a list of tuples (x,y) of float values.</span>
<span class="sd">    :returns: intercept, slope</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">points</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="mf">0.</span>

    <span class="c1"># Fix means</span>
    <span class="n">mean_x</span> <span class="o">=</span> <span class="n">fmean</span><span class="p">([</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">points</span><span class="p">])</span>
    <span class="n">mean_y</span> <span class="o">=</span> <span class="n">fmean</span><span class="p">([</span><span class="n">y</span> <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">points</span><span class="p">])</span>

    <span class="n">xy_sum</span> <span class="o">=</span> <span class="mf">0.</span>
    <span class="n">xsq_sum</span> <span class="o">=</span> <span class="mf">0.</span>
    <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">points</span><span class="p">:</span>
        <span class="n">dx</span> <span class="o">=</span> <span class="n">x</span> <span class="o">-</span> <span class="n">mean_x</span>
        <span class="n">dy</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">mean_y</span>
        <span class="n">xy_sum</span> <span class="o">+=</span> <span class="p">(</span><span class="n">dx</span><span class="o">*</span><span class="n">dy</span><span class="p">)</span>
        <span class="n">xsq_sum</span> <span class="o">+=</span> <span class="p">(</span><span class="n">dx</span><span class="o">*</span><span class="n">dx</span><span class="p">)</span>

    <span class="c1"># Intercept</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">xy_sum</span>
    <span class="k">if</span> <span class="n">xsq_sum</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">m</span> <span class="o">=</span> <span class="n">xy_sum</span> <span class="o">/</span> <span class="n">xsq_sum</span>

    <span class="c1"># Slope</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">mean_y</span> <span class="o">-</span> <span class="n">m</span> <span class="o">*</span> <span class="n">mean_x</span>

    <span class="k">return</span> <span class="n">b</span><span class="p">,</span> <span class="n">m</span></div>

<span class="c1"># ---------------------------------------------------------------------------</span>


<div class="viewcode-block" id="tansey_linear_regression"><a class="viewcode-back" href="../../../rst/calculus.stats.html#calculus.tansey_linear_regression">[docs]</a><span class="k">def</span> <span class="nf">tansey_linear_regression</span><span class="p">(</span><span class="n">points</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Linear regression, as proposed in AnnotationPro.</span>

<span class="sd">    http://annotationpro.org/</span>

<span class="sd">    Translated from C# code from here:</span>
<span class="sd">    https://gist.github.com/tansey/1375526</span>

<span class="sd">    :param points: a list of tuples (x,y) of float values.</span>
<span class="sd">    :returns: intercept, slope</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">points</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="mf">0.</span>

    <span class="n">sum_x_sq</span> <span class="o">=</span> <span class="mf">0.</span>
    <span class="n">sum_codeviates</span> <span class="o">=</span> <span class="mf">0.</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">points</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">points</span><span class="p">:</span>
        <span class="n">sum_codeviates</span> <span class="o">+=</span> <span class="p">(</span><span class="n">x</span><span class="o">*</span><span class="n">y</span><span class="p">)</span>
        <span class="n">sum_x_sq</span> <span class="o">+=</span> <span class="p">(</span><span class="n">x</span><span class="o">*</span><span class="n">x</span><span class="p">)</span>

    <span class="n">sum_x</span> <span class="o">=</span> <span class="n">fsum</span><span class="p">([</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">points</span><span class="p">])</span>
    <span class="n">sum_y</span> <span class="o">=</span> <span class="n">fsum</span><span class="p">([</span><span class="n">y</span> <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">points</span><span class="p">])</span>
    <span class="n">mean_x</span> <span class="o">=</span> <span class="n">fmean</span><span class="p">([</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">points</span><span class="p">])</span>
    <span class="n">mean_y</span> <span class="o">=</span> <span class="n">fmean</span><span class="p">([</span><span class="n">y</span> <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">points</span><span class="p">])</span>

    <span class="n">ssx</span> <span class="o">=</span> <span class="n">sum_x_sq</span> <span class="o">-</span> <span class="p">((</span><span class="n">sum_x</span><span class="o">*</span><span class="n">sum_x</span><span class="p">)</span> <span class="o">/</span> <span class="n">n</span><span class="p">)</span>
    <span class="n">sco</span> <span class="o">=</span> <span class="n">sum_codeviates</span> <span class="o">-</span> <span class="p">((</span><span class="n">sum_x</span> <span class="o">*</span> <span class="n">sum_y</span><span class="p">)</span> <span class="o">/</span> <span class="n">n</span><span class="p">)</span>

    <span class="n">b</span> <span class="o">=</span> <span class="n">mean_y</span> <span class="o">-</span> <span class="p">((</span><span class="n">sco</span> <span class="o">/</span> <span class="n">ssx</span><span class="p">)</span> <span class="o">*</span> <span class="n">mean_x</span><span class="p">)</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">sco</span> <span class="o">/</span> <span class="n">ssx</span>

    <span class="k">return</span> <span class="n">b</span><span class="p">,</span> <span class="n">m</span></div>
</pre></div>

          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../../search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="../../../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="nav-item nav-item-0"><a href="../../../index.html">SPPAS 1.9.8 documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="../../index.html" >Module code</a> &#187;</li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright Copyright (C) 2011-2018 Brigitte Bigi.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.7.6.
    </div>
  </body>
</html>