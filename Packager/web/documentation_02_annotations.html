<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <meta name="author" content="Brigitte Bigi">
  <title>SPPAS Documentation</title>
  <style type="text/css">code{white-space: pre;}</style>
  <!--[if lt IE 9]>
    <script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
  <![endif]-->
  <link rel="stylesheet" href="etc/styles/sppas.css">
      <link rel="stylesheet" type="text/css" href="http://fonts.googleapis.com/css?family=Jura">
      <link rel="stylesheet" type="text/css" href="http://fonts.googleapis.com/css?family=Maven Pro">
      <link rel="icon" href="../etc/icons/sppas.ico" />
  
      <script type="text/javascript">
          var _gaq = _gaq || [];
          _gaq.push(['_setAccount', 'UA-38055333-1']);
          _gaq.push(['_trackPageview']);
  
          (function() {
          var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
          ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
          var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
          })();
      </script>
</head>
<body>

    <div class="header">
        <div class="title">
            <div class="h1title">SPPAS</div>
            <div class="h2title">Automatic Annotation of Speech</div>
            <div class="h3title"><a href="http://www.lpl-aix.fr/~bigi/">Brigitte Bigi</a> - Laboratoire Parole et Langage - Aix-en-Provence - France</div>
        </div>
        <div class="menu">
            <a class="neonBlue"    href="index.html">Home</a>
            <a class="neonGreen"   href="installation.html">Installation</a>
            <a class="neonYellow"  href="download.html">Download</a>
            <a class="neonOrange"  href="documentation.html">Documentation</a>
            <a class="neonRed"     href="https://groups.google.com/forum/?fromgroups#!forum/sppas-users">User's group</a>
        </div>
    </div>
<header>
<h1 class="title">SPPAS Documentation</h1>
<h2 class="author">Brigitte Bigi</h2>
<h3 class="date">Version 1.7.1</h3>
</header>
<nav id="TOC">
<ul>
<li><a href="#automatic-speech-segmentation-and-annotation">Automatic Speech Segmentation and Annotation</a><ul>
<li><a href="#introduction">Introduction</a><ul>
<li><a href="#about-this-chapter">About this chapter</a></li>
<li><a href="#file-formats-and-tier-names">File formats and tier names</a></li>
<li><a href="#recorded-speech">Recorded speech</a></li>
<li><a href="#orthographic-transcription">Orthographic Transcription</a></li>
</ul></li>
<li><a href="#inter-pausal-units-ipus-segmentation">Inter-Pausal Units (IPUs) segmentation</a><ul>
<li><a href="#silencespeech-segmentation">Silence/Speech segmentation</a></li>
<li><a href="#silencespeech-segmentation-time-aligned-with-a-transcription">Silence/Speech segmentation time-aligned with a transcription</a></li>
<li><a href="#split-into-multiple-files">Split into multiple files</a></li>
</ul></li>
<li><a href="#tokenization">Tokenization</a></li>
<li><a href="#phonetization">Phonetization</a></li>
<li><a href="#alignment">Alignment</a></li>
<li><a href="#syllabification">Syllabification</a></li>
<li><a href="#repetitions">Repetitions</a></li>
<li><a href="#momel-and-intsint">Momel and INTSINT</a><ul>
<li><a href="#momel-modelling-melody">Momel (modelling melody)</a></li>
<li><a href="#encoding-of-f0-target-points-using-the-intsint-system">Encoding of F0 target points using the &quot;INTSINT&quot; system</a></li>
</ul></li>
</ul></li>
</ul>
</nav>
<hr />
<h1 id="automatic-speech-segmentation-and-annotation">Automatic Speech Segmentation and Annotation</h1>
<h2 id="introduction">Introduction</h2>
<h3 id="about-this-chapter">About this chapter</h3>
<p>This chapter is not a description on how each automatic annotation is implemented and how it's working: the references are available for that specific purpose!</p>
<p>Instead, this chapter describes how each automatic annotation can be used in SPPAS, i.e. what is the goal of the annotation, what are the requirements, what kind of resources are used, and what is the expected result. Each automatic annotation is then illustrated as a workflow schema, where:</p>
<ul>
<li>blue boxes represent the name of the automatic annotation,</li>
<li>red boxes represent tiers (with their name mentioned in white),</li>
<li>green boxes indicate the resource,</li>
<li>yellow boxes indicate the annotated file (given as input or produced as output).</li>
</ul>
<p>At the end of each automatic annotation process, SPPAS produces a Procedure Outcome Report that aims to be read!</p>
<p>Among others, SPPAS is able to produce automatically annotations from a recorded speech sound and its orthographic transcription. Let us first introduce what is the exaclty meaning of &quot;recorded speech&quot; and &quot;orthographic transcription&quot;.</p>
<h3 id="file-formats-and-tier-names">File formats and tier names</h3>
<p>When using the Graphical User Interface, the file format for input and output can be fixed in the Settings and is applied to all annotations, and file names of each annotation is already fixed and can't be changed. When using the Command-Line interface, or when using scripts, each annotation can be configured independently (file format and file names). In all cases, <strong>the name of the tiers are fixed and can't be changed</strong>!</p>
<h3 id="recorded-speech">Recorded speech</h3>
<p>First of all:</p>
<blockquote>
<p>Only <code>wav</code>, <code>aiff</code> and <code>au</code> audio files and only as mono are supported by SPPAS.</p>
</blockquote>
<p>SPPAS verifies if the wav file is 16 bits and 16000 Hz sample rate. Otherwise it automatically converts to this configuration. For very long files, this may take time. So, the following are possible:</p>
<ol type="1">
<li>be patient</li>
<li>prepare by your own the required wav/mono/16000Hz/16bits files to be used in SPPAS</li>
</ol>
<p>Secondly, a relatively good recording quality is expected. Providing a guideline or recommendation for that is impossible, because it depends: &quot;IPU segmentation&quot; requires a better quality compared to what is expected by &quot;Alignment&quot;, and for that latter, it depends on the language.</p>
<figure>
<img src="./etc/screenshots/signal.png" alt="Example of recorded speech" /><figcaption>Example of recorded speech</figcaption>
</figure>
<h3 id="orthographic-transcription">Orthographic Transcription</h3>
<h4 id="overview">Overview</h4>
<blockquote>
<p>Only UTF-8 encoding is supported by SPPAS.</p>
</blockquote>
<p>Clearly, there are different ways to pronounce the same utterance. Different speakers have different accents and tend to speak at different rates. There are commonly two types of Speech Corpora. First is related to “Read Speech” which includes book excerpts, broadcast news, lists of words, sequences of numbers. Second is often named as “Spontaneous Speech” which includes dialogs - between two or more people (includes meetings), narratives - a person telling a story, map- tasks - one person explains a route on a map to another, appointment-tasks - two people try to find a common meeting time based on individual schedules. One of the characteristics of Spontaneous Speech is an important gap between a word’s phonological form and its phonetic realizations. Specific realization due to elision or reduction processes are frequent in spontaneous data. It also presents other types of phenomena such as non-standard elisions, substitutions or addition of phonemes which intervene in the automatic phonetization and alignment tasks.</p>
<p>Consequently, when a speech corpus is transcribed into a written text, the transcriber is immediately confronted with the following question: how to reflect the orality of the corpus? <em>Transcription conventions</em> are then designed to provide rules for writing speech corpora. These conventions establish phenomena to transcribe and also how to annotate them.</p>
<p>In that sense, the orthographic transcription must be a representation of what is “perceived” in the signal. Consequently, it <strong>must</strong> includes:</p>
<ul>
<li>filled pauses;</li>
<li>short pauses;</li>
<li>repeats;</li>
<li>noises and laugh items (not available for: English, Japanese and Cantonese).</li>
</ul>
<p>In speech (particularly in spontaneous speech), many phonetic variations occur. Some of these phonologically known variants are predictable and can be included in the pronunciation dictionary but many others are still unpredictable (especially invented words, regional words or words borrowed from another language).</p>
<blockquote>
<p>SPPAS is the only automatic annotation software that deals with <strong>Enriched Orthographic Transcriptions</strong>.</p>
</blockquote>
<h4 id="convention">Convention</h4>
<p>The transcription must use the following convention:</p>
<ul>
<li>truncated words, noted as a ’-’ at the end of the token string (an ex- example);</li>
<li>noises, noted by a ’*’ (not available for: English, Japanese and Cantonese);</li>
<li>laughs, noted by a ’@’ (not available for: English, Japanese and Cantonese);</li>
<li>short pauses, noted by a ’+’;</li>
<li>elisions, mentioned in parenthesis;</li>
<li>specific pronunciations, noted with brackets [example,eczap];</li>
<li>comments are noted inside braces or brackets without using comma {this} or [this and this];</li>
<li>liaisons, noted between ’=’ (an =n= example);</li>
<li>morphological variants with &lt;like,lie ok&gt;,</li>
<li>proper name annotation, like $John S. Doe$.</li>
</ul>
<p>SPPAS also allows to include in the transcription:</p>
<ul>
<li>regular punctuations,</li>
<li>numbers: they will be automatically converted to their written form.</li>
</ul>
<p>The result is what we call an enriched orthographic construction, from which two derived transcriptions are generated automatically: the <strong>standard transcription</strong> (the list of orthographic tokens) and a specific transcription from which the phonetic tokens are obtained to be used by the grapheme-phoneme converter that is named <strong>faked transcription</strong>.</p>
<h4 id="example">Example</h4>
<p><em>This is + hum... an enrich(ed) transcription {loud} number 1!</em></p>
<p>The derived transcriptions are:</p>
<ul>
<li>standard: this is hum an enriched transcription number one</li>
<li>faked: this is + hum an enrich transcription number one</li>
</ul>
<h2 id="inter-pausal-units-ipus-segmentation">Inter-Pausal Units (IPUs) segmentation</h2>
<p>The &quot;IPUs segmentation&quot; automatic annotation can perform 3 actions:</p>
<ol type="1">
<li>find silence/speech segmentation of a recorded file,</li>
<li>find silence/speech segmentation of a recorded file including the time-alignment of utterances of a transcription file,</li>
<li>split/save a recorded file into multiple files, depending on segments indicated in a time-aligned file.</li>
</ol>
<figure>
<img src="./etc/figures/segmworkflow.bmp" alt="IPU Segmentation workflow" /><figcaption>IPU Segmentation workflow</figcaption>
</figure>
<h3 id="silencespeech-segmentation">Silence/Speech segmentation</h3>
<p>The IPUs Segmentation annotation performs a silence detection from a recorded file. This segmentation provides an annotated file with one tier named &quot;IPU&quot;. The silence intervals are labelled with the &quot;#&quot; symbol, as speech intervals are labelled with &quot;ipu_&quot; followed by the IPU number.</p>
<p>The following parameters must be fixed:</p>
<ul>
<li><p>Minimum volume value (in seconds):<br />If this value is set to zero, the minimum volume is automatically adjusted for each sound file. Try with it first, then if the automatic value is not correct, fix it manually. The Procedure Outcome Report indicates the value the system choose. The SndRoamer component can also be of great help: it indicates min, max and mean volume values of the sound.</p></li>
<li><p>Minimum silence duration (in seconds): By default, this is fixed to 0.2 sec., an appropriate value for French. This value should be at least 0.25 sec. for English.</p></li>
<li><p>Minimum speech duration (in seconds): By default, this value is fixed to 0.3 sec. The most relevent value depends on the speech style: for isolated sentences, probably 0.5 sec should be better, but it should be about 0.1 sec for spontaneous speech.</p></li>
<li><p>Speech boundary shift (in seconds): a duration which is systematically added to speech boundaries, to enlarge the speech interval.</p></li>
</ul>
<p>The procedure outcome report indicates the values (volume, minimum durations) that was used by the system for each sound file given as input. It also mentions the name of the output file (the resulting file). The file format can be fixed in the Settings of SPPAS (xra, TextGrid, eaf, ...).</p>
<figure>
<img src="./etc/screenshots/ipu-seg-log.png" alt="Procedure outcome report of IPUs Segmentation" /><figcaption>Procedure outcome report of IPUs Segmentation</figcaption>
</figure>
<p>The annotated file can be checked manually (preferably in Praat than Elan nor Anvil). If such values was not correct, then, delete the annotated file that was previously created, change the default values and re-annotate.</p>
<figure>
<img src="./etc/screenshots/ipu-seg-result1.png" alt="Result of IPUs Segmentation: silence detection" /><figcaption>Result of IPUs Segmentation: silence detection</figcaption>
</figure>
<p>Notice that the speech segments can be transcribed using the &quot;IPUScribe&quot; component.</p>
<figure>
<img src="./etc/screenshots/IPUscribe-2.png" alt="Orthographic transcription based on IPUs Segmentation" /><figcaption>Orthographic transcription based on IPUs Segmentation</figcaption>
</figure>
<h3 id="silencespeech-segmentation-time-aligned-with-a-transcription">Silence/Speech segmentation time-aligned with a transcription</h3>
<p>Inter-Pausal Units segmentation can also consist in aligning macro-units of a document with the corresponding sound.</p>
<p>SPPAS identifies silent pauses in the signal and attempts to align them with the inter-pausal units proposed in the transcription file, under the assumption that each such unit is separated by a silent pause. This algorithm is language-independent: it can work on any language.</p>
<p>In the transcription file, <strong>silent pauses must be indicated</strong> using both solutions, which can be combined:</p>
<ul>
<li>with the symbol '#',</li>
<li>with newlines.</li>
</ul>
<p>A recorded speech file must strictly correspond to a transcription, except for the extension expected as .txt for this latter. The segmentation provides an annotated file with one tier named &quot;IPU&quot;. The silence intervals are labelled with the &quot;#&quot; symbol, as speech intervals are labelled with &quot;ipu_&quot; followed by the IPU number then the corresponding transcription.</p>
<p>The same parameters than those indicated in the previous section must be fixed.</p>
<pre><code>Important: 
This segmentation was tested on documents no longer than one paragraph 
(about 1 minute speech). </code></pre>
<figure>
<img src="./etc/screenshots/ipu-seg-result2.png" alt="Silence/Speech segmentation with orthographic transcription" /><figcaption>Silence/Speech segmentation with orthographic transcription</figcaption>
</figure>
<h3 id="split-into-multiple-files">Split into multiple files</h3>
<p>IPU segmentation can split the sound into multiple files (one per IPU), and it creates a text file for each of the tracks. The output file names are &quot;track_0001&quot;, &quot;track_0002&quot;, etc.</p>
<p>Optionally, if the input annotated file contains a tier named exactly &quot;Name&quot;, then the content of this tier will be used to fix output file names.</p>
<figure>
<img src="./etc/screenshots/ipu-seg-result3.png" alt="Data to split" /><figcaption>Data to split</figcaption>
</figure>
<p>In the example above, the automatic process will create 6 files: FLIGTH.wav, FLIGHT.txt, MOVIES.wav, MOVIES.txt, SLEEP.wav and SLEEP.txt. It is up to the user to perform another IPU segmentation of these files to get another file format than txt (xra, TextGrid, ...) thanks to the previous section &quot;Silence/Speech segmentation time-aligned with a transcription&quot;.</p>
<figure>
<img src="./etc/screenshots/ipu-seg-result3-files.png" alt="Data split" /><figcaption>Data split</figcaption>
</figure>
<h2 id="tokenization">Tokenization</h2>
<p>Tokenization is also known as &quot;Text Normalization&quot; the process of segmenting a text into tokens. In principle, any system that deals with unrestricted text need the text to be normalized. Texts contain a variety of &quot;non-standard&quot; token types such as digit sequences, words, acronyms and letter sequences in all capitals, mixed case words, abbreviations, roman numerals, URL's and e-mail addresses... Normalizing or rewriting such texts using ordinary words is then an important issue. The main steps of the text normalization proposed in SPPAS are:</p>
<ul>
<li>Remove punctuation;</li>
<li>Lower the text;</li>
<li>Convert numbers to their written form;</li>
<li>Replace symbols by their written form, thanks to a &quot;replacement&quot; dictionary, located into the sub-directory &quot;repl&quot; in the &quot;resources&quot; directory. Do not hesitate to add new replacements in this dictionary.</li>
<li>Word segmentation based on the content of a lexicon. If the result is not corresponding to your expectations, fill free to modify the lexicon, located in the &quot;vocab&quot; sub-directory of the &quot;resources&quot; directory. The lexicon contains one word per line.</li>
</ul>
<p>For more details, see the following reference:</p>
<blockquote>
<p><strong>Brigitte Bigi (2011).</strong> <em>A Multilingual Text Normalization Approach.</em> 2nd Less-Resourced Languages workshop, 5th Language Technology Conference, Poznàn (Poland).</p>
</blockquote>
<figure>
<img src="./etc/figures/tokworkflow.bmp" alt="Text normalization workflow" /><figcaption>Text normalization workflow</figcaption>
</figure>
<p>The SPPAS Tokenization system takes as input a file including a tier with the orthographic transcription. The name of this tier must contains one of the following strings:</p>
<ul>
<li>trs</li>
<li>trans</li>
<li>ipu</li>
<li>ortho</li>
<li>toe</li>
</ul>
<p>The first tier that matches is used (case insensitive search).</p>
<p>By default, it produces a file including only one tier with the tokens. To get both transcription tiers faked and standard, check such option!</p>
<ul>
<li>Tokens-std: the text normalization of the standard transcription,</li>
<li>Tokens-faked: the text normalization of the faked transcription.</li>
</ul>
<p>Read the &quot;Introduction&quot; of this chapter to understand the difference between &quot;standard&quot; and &quot;faked&quot; transcriptions.</p>
<h2 id="phonetization">Phonetization</h2>
<p>Phonetization, also called grapheme-phoneme conversion, is the process of representing sounds with phonetic signs.</p>
<p>SPPAS implements a dictionary based-solution which consists in storing a maximum of phonological knowledge in a lexicon. In this sense, this approach is language-independent. SPPAS phonetization process is the equivalent of a sequence of dictionary look-ups.</p>
<p>The SPPAS phonetization takes as input an orthographic transcription previously normalized (by the Tokenization automatic system or manually). The name of this tier must contains one of the following strings:</p>
<ul>
<li>&quot;tok&quot; and &quot;std&quot;</li>
<li>&quot;tok&quot; and &quot;faked&quot;</li>
</ul>
<p>The first tier that matches is used (case insensitive search).</p>
<p>The system produces a phonetic transcription.</p>
<figure>
<img src="./etc/figures/phonworkflow.bmp" alt="Phonetization workflow" /><figcaption>Phonetization workflow</figcaption>
</figure>
<p>Actually, some words can correspond to several entries in the dictionary with various pronunciations, all these variants are stored in the phonetization result. By convention, spaces separate words, dots separate phones and pipes separate phonetic variants of a word. For example, the transcription utterance:</p>
<ul>
<li>Tokenization: <code>the flight was twelve hours long</code></li>
<li>Phonetization: <code>dh.ax|dh.ah|dh.iy f.l.ay.t w.aa.z|w.ah.z|w.ax.z|w.ao.z t.w.eh.l.v aw.er.z|aw.r.z l.ao.ng</code></li>
</ul>
<p>Many of the other systems assume that all words of the speech transcription are mentioned in the pronunciation dictionary. On the contrary, SPPAS includes a language-independent algorithm which is able to phonetize unknown words of any language as long as a dictionary is available! If such case occurs during the phonetization process, a WARNING mentions it in the Procedure Outcome Report.</p>
<p>For details, see the following reference:</p>
<blockquote>
<p><strong>Brigitte Bigi (2013).</strong> <em>A phonetization approach for the forced-alignment task</em>, 3rd Less-Resourced Languages workshop, 6th Language &amp; Technology Conference, Poznan (Poland).</p>
</blockquote>
<p>Since the phonetization is only based on the use of a pronunciation dictionary, the quality of such a phonetization only depends on this resource. If a pronunciation is not as expected, it is up to the user to change it in the dictionary. All dictionaries are located in the sub-directory &quot;dict&quot; of the &quot;resources&quot; directory.</p>
<p>SPPAS uses the same dictionary-format as proposed in VoxForge, i.e. the HTK ASCII format. Here is a peace of the eng.dict file:</p>
<pre><code>    THE             [THE]           D @
    THE(2)          [THE]           D V
    THE(3)          [THE]           D i:
    THEA            [THEA]          T i: @
    THEALL          [THEALL]        T i: l
    THEANO          [THEANO]        T i: n @U
    THEATER         [THEATER]       T i: @ 4 3:r
    THEATER&#39;S       [THEATER&#39;S]     T i: @ 4 3:r z</code></pre>
<p>The first column indicates the word, followed by the variant number (except for the first one). The second column indicated the word between brackets. The last columns are the succession of phones, separated by a whitespace.</p>
<h2 id="alignment">Alignment</h2>
<p>Alignment, also called phonetic segmentation, is the process of aligning speech with its corresponding transcription at the phone level. The alignment problem consists in a time-matching between a given speech unit along with a phonetic representation of the unit.</p>
<p>SPPAS is based on the Julius Speech Recognition Engine (SRE). Speech Alignment also requires an Acoustic Model in order to align speech. An acoustic model is a file that contains statistical representations of each of the distinct sounds of one language. Each phoneme is represented by one of these statistical representations. SPPAS is working with HTK-ASCII acoustic models, trained from 16 bits, 16000 Hz wav files.</p>
<p>Speech segmentation was evaluated for French: in average, automatic speech segmentation is 95% of times within 40ms compared to the manual segmentation (tested on read speech and on conversational speech). Details about these results are available in the slides of the following reference:</p>
<blockquote>
<p><em>Brigitte Bigi</em> (2014). <strong>Automatic Speech Segmentation of French: Corpus Adaptation</strong>. 2nd Asian Pacific Corpus Linguistics Conference, p. 32, Hong Kong.</p>
</blockquote>
<figure>
<img src="./etc/figures/alignworkflow.bmp" alt="Alignment workflow" /><figcaption>Alignment workflow</figcaption>
</figure>
<p>The SPPAS aligner takes as input the phonetization and optionally the tokenization. The name of the phonetization tier must contains the string &quot;phon&quot;. The first tier that matches is used (case insensitive search).</p>
<p>The annotation provides one annotated file with 3 tiers:</p>
<ul>
<li>&quot;PhonAlign&quot;, is the segmentation at the phone level;</li>
<li>&quot;PhnTokAlign&quot; is the segmentation at the word level, with phonemes as labels;</li>
<li>&quot;TokensAlign&quot; is the segmentation at the word level.</li>
</ul>
<figure>
<img src="./etc/screenshots/alignment.png" alt="SPPAS alignment output example" /><figcaption>SPPAS alignment output example</figcaption>
</figure>
<p>The following options are available to configure alignment:</p>
<ul>
<li>Expend option: If expend is checked, SPPAS will expend the last phoneme and the last token of each unit to the unit duration.</li>
<li>Extend option: If extend is checked, SPPAS will extend the last phoneme and the last token to the wav duration, otherwise SPPAS adds a silence.</li>
<li>Remove temporary files: keep only alignment result and remove intermediary files (they consists in one wav file per unit and a set of text files per unit).</li>
<li>Speech segmentation system can be either: julius, hvite or basic</li>
<li>Guess short pauses after each token</li>
</ul>
<h2 id="syllabification">Syllabification</h2>
<p>The syllabification of phonemes is performed with a rule-based system from time-aligned phonemes. This phoneme-to-syllable segmentation system is based on 2 main principles:</p>
<ul>
<li>a syllable contains a vowel, and only one;</li>
<li>a pause is a syllable boundary.</li>
</ul>
<p>These two principles focus the problem of the task of finding a syllabic boundary between two vowels. As in state-of-the-art systems, phonemes were grouped into classes and rules established to deal with these classes. We defined general rules followed by a small number of exceptions. Consequently, the identification of relevant classes is important for such a system.</p>
<p>We propose the following classes, for both French and Italian set of rules:</p>
<ul>
<li>V - Vowels,</li>
<li>G - Glides,</li>
<li>L - Liquids,</li>
<li>O - Occlusives,</li>
<li>F - Fricatives,</li>
<li>N - Nasals.</li>
</ul>
<p>The rules we propose follow usual phonological statements for most of the corpus. A configuration file indicates phonemes, classes and rules. This file can be edited and modified to adapt the syllabification.</p>
<p>For more details, see the following reference:</p>
<blockquote>
<p><strong>B. Bigi, C. Meunier, I. Nesterenko, R. Bertrand</strong> (2010). <em>Automatic detection of syllable boundaries in spontaneous speech.</em> Language Resource and Evaluation Conference, pp 3285-3292, La Valetta, Malte.</p>
</blockquote>
<figure>
<img src="./etc/figures/syllworkflow.bmp" alt="Syllabification workflow" /><figcaption>Syllabification workflow</figcaption>
</figure>
<p>The Syllabification annotation takes as input one file with (at least) one tier containing the time-aligned phonemes. The annotation provides one annotated file with 3 tiers (Syllables, Classes and Structures).</p>
<figure>
<img src="./etc/screenshots/syll-example.png" alt="Syllabification example" /><figcaption>Syllabification example</figcaption>
</figure>
<p>If the syllabification is not as expected, you can change the set of rules. The configuration file is located in the sub-directory &quot;syll&quot; of the &quot;resources&quot; directory.</p>
<p>The syllable configuration file is a simple ASCII text file that any user can change as needed. At first, the list of phonemes and the class symbol associated with each of the phonemes are described as, for example:</p>
<ul>
<li><code>PHONCLASS e V</code></li>
<li><code>PHONCLASS p O</code></li>
</ul>
<p>The couples phoneme/class are made of 3 columns: the first column is the key-word PHONCLASS, the second column is the phoneme symbol, the third column is the class symbol.The constraints on this definition are:</p>
<ul>
<li>a pause is mentioned with the class-symbol #,</li>
<li>a class-symbol is only one upper-case character, except:
<ul>
<li>the character X if forbidden;</li>
<li>the characters V and W are used for vowels.</li>
</ul></li>
</ul>
<p>The second part of the configuration file contains the rules. The first column is a keyword, the second column describes the classes between two vowels and the third column is the boundary location. The first column can be:</p>
<ul>
<li><code>GENRULE</code>,</li>
<li><code>EXCRULE</code>, or</li>
<li><code>OTHRULE</code>.</li>
</ul>
<p>In the third column, a 0 means the boundary is just after the first vowel, 1 means the boundary is one phoneme after the first vowel, etc. Here are some examples, corresponding to the rules described in this paper for spontaneous French:</p>
<ul>
<li><code>GENRULE VXV 0</code></li>
<li><code>GENRULE VXXV 1</code></li>
<li><code>EXCRULE VFLV 0</code></li>
<li><code>EXCRULE VOLGV 0</code></li>
</ul>
<p>Finally, to adapt the rules to specific situations that the rules failed to model, we introduced some phoneme sequences and the boundary definition. Specific rules contain only phonemes or the symbol &quot;ANY&quot; which means any phoneme. It consists of 7 columns: the first one is the key-word OTHRULE, the 5 following columns are a phoneme sequence where the boundary should be applied to the third one by the rules, the last column is the shift to apply to this boundary. In the following example:</p>
<p><code>OTHRULE ANY ANY p s k -2</code></p>
<h2 id="repetitions">Repetitions</h2>
<p>This automatic detection focus on word repetitions, which can be an exact repetition (named strict echo) or a repetition with variation (named non-strict echo).</p>
<p>SPPAS implements <em>self-repetitions</em> and <em>other-repetitions</em> detection. The system is based only on lexical criteria. The proposed algorithm is focusing on the detection of the source.</p>
<p>The Graphical User Interface only allows to detect self-repetitions. Use the Command-Line User Interface if you want to get other-repetitions.</p>
<p>For more details, see the following paper:</p>
<blockquote>
<p><strong>Brigitte Bigi, Roxane Bertrand, Mathilde Guardiola</strong> (2014). <em>Automatic detection of other-repetition occurrences: application to French conversational speech</em>, 9th International conference on Language Resources and Evaluation (LREC), Reykjavik (Iceland).</p>
</blockquote>
<figure>
<img src="./etc/figures/repetworkflow.bmp" alt="Repetition detection workflow" /><figcaption>Repetition detection workflow</figcaption>
</figure>
<p>The automatic annotation takes as input a file with (at least) one tier containing the time-aligned tokens of the speaker (and another file/tier for other-repetitions). The annotation provides one annotated file with 2 tiers (Sources and Repetitions).</p>
<p>This process requires a list of stop-words, and a dictionary with lemmas (the system can process without it, but the result is better with it). Both lexicons are located in the &quot;vocab&quot; sub-directory of the &quot;resources&quot; directory.</p>
<h2 id="momel-and-intsint">Momel and INTSINT</h2>
<h3 id="momel-modelling-melody">Momel (modelling melody)</h3>
<p>Momel is an algorithm for the automatic modelling of fundamental frequency (F0) curves using a technique called assymetric modal quaratic regression.</p>
<p>This technique makes it possible by an appropriate choice of parameters to factor an F0 curve into two components:</p>
<ul>
<li>a macroprosodic component represented by a a quadratic spline function defined by a sequence of target points &lt; ms, hz &gt;.</li>
<li>a microprosodic component represented by the ratio of each point on the F0 curve to the corresponding point on the quadratic spline function.</li>
</ul>
<p>The algorithm which we call Asymmetrical Modal Regression comprises the following four stages:</p>
<p>For details, see the following reference:</p>
<blockquote>
<p><strong>Daniel Hirst and Robert Espesser</strong> (1993). <em>Automatic modelling of fundamental frequency using a quadratic spline function.</em> Travaux de l’Institut de Phonétique d’Aix. vol. 15, pages 71-85.</p>
</blockquote>
<p>The SPPAS implementation of Momel requires a file with the F0 values, sampled at 10 ms. Two extensions are supported:</p>
<ul>
<li>.PitchTier, from Praat.</li>
<li>.hz, from any tool, is a file with one F0 value per line.</li>
</ul>
<p>These options can be fixed:</p>
<ul>
<li>Window length used in the &quot;cible&quot; method</li>
<li>F0 threshold: Maximum F0 value</li>
<li>F0 ceiling: Minimum F0 value</li>
<li>Maximum error: Acceptable ratio between two F0 values</li>
<li>Window length used in the &quot;reduc&quot; method</li>
<li>Minimal distance</li>
<li>Minimal frequency ratio</li>
<li>Eliminate glitch option: Filter f0 values before 'cible'</li>
</ul>
<h3 id="encoding-of-f0-target-points-using-the-intsint-system">Encoding of F0 target points using the &quot;INTSINT&quot; system</h3>
<p>INTSINT assumes that pitch patterns can be adequately described using a limited set of tonal symbols, T,M,B,H,S,L,U,D (standing for : Top, Mid, Bottom, Higher, Same, Lower, Up-stepped, Down-stepped respectively) each one of which characterises a point on the fundamental frequency curve.</p>
<p>The rationale behind the INTSINT system is that the F0 values of pitch targets are programmed in one of two ways : either as absolute tones T, M, B which are assumed to refer to the speaker’s overall pitch range (within the current Intonation Unit), or as relative tones H, S, L, U, D assumed to refer only to the value of the preceding target point.</p>
<figure>
<img src="./etc/img/INTSINT-tones.png" alt="INTSINT example" /><figcaption>INTSINT example</figcaption>
</figure>
<p>The rationale behind the INTSINT system is that the F0 values of pitch targets are programmed in one of two ways : either as absolute tones T, M, B which are assumed to refer to the speaker’s overall pitch range (within the current Intonation Unit), or as relative tones H, S, L, U, D assumed to refer only to the value of the preceding target point.</p>
<p>A distinction is made between non-iterative H, S, L and iterative U, D relative tones since in a number of descriptions it appears that iterative raising or lowering uses a smaller F0 interval than non-iterative raising or lowering. It is further assumed that the tone S has no iterative equivalent since there would be no means of deciding where intermediate tones are located.</p>
<figure>
<img src="./etc/screenshots/Momel-INTSINT.png" />
</figure>
<blockquote>
<p><strong>D.-J. Hirst</strong> (2011). <em>The analysis by synthesis of speech melody: from data to models</em>, Journal of Speech Sciences, vol. 1(1), pages 55-83.</p>
</blockquote>
<hr />

    <div id="footer">
        <a href="index.html"><img src="./etc/logos/sppas-logo.png" alt="SPPAS"></a>
        <p class="copyright">
            <a href="mailto:brigitte.bigi@gmail.com">Brigitte Bigi</a> &copy; 2011-2015
        </p>
    </div>

    <span class="scrollT"></span>
    <script type="text/javascript" src="./etc/scripts/fn.scrollT.js"></script>
    <script type="text/javascript" src="./etc/scripts/fn.center.js"></script>
    <script type="text/javascript" src="./etc/scripts/main.js"></script>
</body>
</html>
